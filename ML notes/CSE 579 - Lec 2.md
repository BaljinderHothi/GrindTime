# Reinforcement Learning Autumn 2024 Lecture #2
https://www.youtube.com/watch?v=ToLxhUoM-oI
https://courses.cs.washington.edu/courses/cse579/24au/lectures/Lecture2_Intro_SL.pdf


### Lecture by Abhishek Gupta
### Ta: Jacob Berg

What is an MDP + why should we care?
- Markov Decision Process
- Utilized to generate labeled data
- It has some key parts:
    - States(s): the situation the Agent can be in
    - Actions(a): The choices the Agent can make
    - Transition Function(t): The probability of moving to a new state given the current state and action
    - Reward Function(r): The immediate reward recieved after taking an action
    - Policy (pi): A mapping from states to actions(what the agent learns)

In supervised imitation learning, the dataset is often generated by an expert acting in an MDP

### Back to the Lecture:

So an informal definition or reinforcement learning is:
- **Using Trial and error in an environment to learn a strategy to maximize some notion of a reward**

```mermaid 
flowchart LR
    Agent[Agent] -->|Action | Environment[Environment]
    Environment -->|States , Rewards | Agent
```

Easy way for agents to continue improving their own behavior on deployment


## Framework for RL - Markov Decision Process
Augment Markov Chain with rewards and actions

Need to look into this more:
- States (S) 
    - The different sitautions the agent can be in
- Actions (A)
    - All of the possible choices that the agent can make
- Rewards (R)
    - The score or feedback that the agent gets after an action
- Initial State Distribution
    - The probability of starting in each state(some states might be more likely to start in)
- Discount (V)
    - A number between 0 and 1 that says how much the agent values future rewards (closer to 1 = more patient)
- Transition Dynamics
    - The probability of ending up in state t+1 is if we are in state t and take action t


(look at images/markov decision process.png for a better visual)

### Markov Property
Your past is independent of your future conditional on your present

The future depends only on the present, not the past.

In other words:

Once you know the current state, you don’t need to know the whole history to predict what happens next.


### Trajectory 
Trajectory is the full sequence of what happens during one episode of interaction between the agent and its environment

so for example the expression

```τ = (s₀, a₀, r₀, s₁, a₁, r₁, ..., sₜ, aₜ, rₜ)```

means 

- The agent starts in state s₀
- Takes action a₀
- Receives reward r₀
- Lands in next state s₁
- Takes action a₁
- Gets reward r₁
- … and so on, until the final timestep T


The Goal of Reinforcement Learning is to the paramaters by policy by theta

Our policy by theta is gonna try and maximize the expectation over the sum of rewards in any trajectory

# Main things to learn - Policies
## Gaussian Policy
Used for continuous action spaces.

The policy outputs the mean μ(s) and covariance Σ(s) of a Gaussian distribution over actions for a given state s.

Common in policy gradient methods like PPO or TRPO.

Action: a ~ N(μ(s), Σ(s))

## Categorical Policy
Used for discrete action spaces.

The policy outputs a probability for each possible action p₁(s), p₂(s), ..., pₖ(s).

Action: sample from the categorical distribution defined by those probabilities.

Example: a ~ Cat(p₁(s), ..., pₖ(s))

## Mixture of Gaussians
A more flexible version of the Gaussian policy.

Outputs N different Gaussians each with their own parameters (μᵢ(s), Σᵢ(s)) and associated weights wᵢ.

Captures multi-modal action distributions (e.g., when multiple distinct strategies could work).

Action: sampled from the weighted mixture of Gaussians.

## Diffusion Policy
A recent innovation, especially useful in high-dimensional or structured action spaces.

Instead of directly parameterizing a distribution, it models the action distribution as the solution to a diffusion process (like a learned denoising process).

Based on score-based models (e.g., from generative modeling literature like diffusion models).

The gradient shown: ∇ₐ log π(a|s) is a score function, indicating how much to adjust action a to increase its likelihood under π.


Easier to code and easier to debug the Guassians

To sample from a Diffusion Policy you have to reverse the fusion process so it is much slower and harder to train, though it is getting better now


## Imitation Learning
- Use expert demonstrations
- Learn policy from (s, a) pairs

### Pros
- No reward engineering
- Easy to specify tasks

### Cons
- Needs expert data
- No learning post-deployment

---

## Behavior Cloning (BC)
- Supervised learning on expert (s, a) pairs
- Maximizes log likelihood:
```math
argmax_θ ∑ log π_θ(a|s)
```

---

## BC Limitations
### Compounding Error
- Mistakes accumulate over time
- Error grows with time horizon

### Underfitting
- Expert policy may be multimodal
- Gaussian/categorical may average modes

---

## Distribution Expressivity
- **Categorical**: For discrete actions
- **Gaussian**: For continuous actions
- **Mixture of Gaussians**: More flexible
- **Diffusion Models**: High-dimensional/multimodal

---

## Divergence Objectives
- **Forward KL** (used in BC): mode-averaging
- **Reverse KL**: mode-seeking
- **f-divergence** framework: generalizes both

---

## Addressing Expressivity
- Mix. of Gaussians
- Latent Variable Models
- Autoregressive Discretization
- Diffusion Models

---

## Fixing Compounding Error
### DAgger (2011)
- Dataset aggregation
- Get expert corrections during rollout

### DART (2017)
- Inject noise into expert to improve robustness

### CCIL
- Model dynamics
- Relabel near-expert states using dynamics model

---

## Diffusion Policy Details
- Models action as denoising process
- Uses ∇ₐ log π(a|s) as score function
- Accurate but expensive to sample
